{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e743d96b-981d-4a8a-9ae1-0e1758c1333f",
   "metadata": {},
   "source": [
    "# RNNs to Transformers\n",
    "This notebook shows how to create models that translate from french to english\n",
    "\n",
    "I will slowly build up the complexity of the model:\n",
    "1. Starting with a seq2seq architecture without attention,\n",
    "2. Adding in attention,\n",
    "3. Then using a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a666a61-e9de-440e-91c0-7b2fa7fecb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518938d-7ad0-4014-99f0-a3e15805fcce",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Using the dataset/data prep code in this tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c935181-74b7-44c4-a3cb-77e3de097da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2814k  100 2814k    0     0  27.2M      0 --:--:-- --:--:-- --:--:-- 27.2M\n"
     ]
    }
   ],
   "source": [
    "!curl https://download.pytorch.org/tutorial/data.zip --output data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd8427ec-740c-4ade-9fcd-9753a004fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "  inflating: ./data/eng-fra.txt      \n",
      "   creating: ./data/names/\n",
      "  inflating: ./data/names/Arabic.txt  \n",
      "  inflating: ./data/names/Chinese.txt  \n",
      "  inflating: ./data/names/Czech.txt  \n",
      "  inflating: ./data/names/Dutch.txt  \n",
      "  inflating: ./data/names/English.txt  \n",
      "  inflating: ./data/names/French.txt  \n",
      "  inflating: ./data/names/German.txt  \n",
      "  inflating: ./data/names/Greek.txt  \n",
      "  inflating: ./data/names/Irish.txt  \n",
      "  inflating: ./data/names/Italian.txt  \n",
      "  inflating: ./data/names/Japanese.txt  \n",
      "  inflating: ./data/names/Korean.txt  \n",
      "  inflating: ./data/names/Polish.txt  \n",
      "  inflating: ./data/names/Portuguese.txt  \n",
      "  inflating: ./data/names/Russian.txt  \n",
      "  inflating: ./data/names/Scottish.txt  \n",
      "  inflating: ./data/names/Spanish.txt  \n",
      "  inflating: ./data/names/Vietnamese.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31748a5a-275f-462e-a4cf-10d2d1915f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4345\n",
      "eng 2803\n",
      "['je suis en train de dormir .', 'i m sleeping .']\n"
     ]
    }
   ],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\" : 1, \"EOS\" : 2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            idx = self.n_words+1\n",
    "            self.word2index[word] = idx\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[idx] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) # adds a space before puncutation, because there has to be a space between each token.  \n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) # replaces punctuation (e.g. apostrophes) with a space\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "french_lang, english_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50def46-6342-4cd6-97de-bf88e7803602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(lang, sentence):\n",
    "    sentence = [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    sentence.append(lang.word2index[\"EOS\"])\n",
    "    return torch.tensor(sentence, dtype=torch.long)\n",
    "\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, lang1, lang2, pairs):\n",
    "        self.lang1 = lang1\n",
    "        self.lang2 = lang2\n",
    "        self.pairs = []\n",
    "        for i in range(len(pairs)):\n",
    "            s1, s2 = pairs[i]\n",
    "            self.pairs.append([convert_to_tensor(lang1, s1), convert_to_tensor(lang2,s2)])\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx][0], self.pairs[idx][1]\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "def pad_collate(batch):\n",
    "    x, y = zip(*batch)\n",
    "    x_lens = torch.tensor([len(i) for i in x])\n",
    "    y_lens = torch.tensor([len(i) for i in y])\n",
    "    x_pad = pad_sequence(x, padding_value=0)\n",
    "    y_pad = pad_sequence(y, padding_value=0)\n",
    "    \n",
    "    # sort \n",
    "    x_lens, idx = x_lens.sort(0, descending=True)\n",
    "    y_lens = y_lens[idx]\n",
    "    x_pad = x_pad[:, idx]\n",
    "    y_pad = y_pad[:, idx]\n",
    "    \n",
    "    return x_pad, y_pad, x_lens, y_lens\n",
    "\n",
    "dataset = SentenceDataset(french_lang, english_lang, pairs)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc52295-f136-456e-ba87-132cb3e63d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca va .', 'i m ok .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "564c3fd2-5af5-48fe-b11c-60c56d9996ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 7, 12, 22,  6,  2]), tensor([ 3,  4, 13,  5,  2]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641bc5d1-7ae9-4e6c-95ba-de9e4f83f3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7,  3,  7,  7,  7,  7,  7,  7,  7,  7, 36,  3,  7,  7,  7,  7,  7,  7,\n",
       "          25,  7,  7,  7,  7,  7,  7,  7,  7,  7,  3, 28, 10, 10],\n",
       "         [12, 15,  8, 12, 12, 12, 12, 12, 12,  8, 11,  4, 12, 12, 12, 12, 12, 12,\n",
       "          26, 12, 12, 12, 12, 12, 12, 12, 12, 12,  4, 29, 11, 11],\n",
       "         [15,  4,  9, 41, 40, 39, 38, 37, 37,  9,  9, 34, 33, 32, 31, 30, 27, 13,\n",
       "          23, 24, 23, 22, 21, 20, 19, 17, 14, 13,  5,  6,  6,  6],\n",
       "         [16, 35,  6,  6, 18, 18,  6,  6, 18,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "           6,  6,  6,  6,  6,  6, 18, 18,  6,  6,  6,  2,  2,  2],\n",
       "         [ 6,  6,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  0,  0],\n",
       "         [ 2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]),\n",
       " tensor([[ 3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "          15,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3],\n",
       "         [ 4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4, 17,\n",
       "          16,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4],\n",
       "         [ 8, 23,  6, 27, 26, 26, 25, 25, 25, 24, 24, 22, 21, 20, 20, 19, 18,  7,\n",
       "          14, 14, 14, 13, 12, 11,  9,  9,  7,  7,  5, 18, 24,  6],\n",
       "         [ 5,  5,  5,  5,  5,  5,  5,  5, 10,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "           5,  5,  5,  5,  5,  5, 10, 10,  5,  5,  2,  5,  5,  5],\n",
       "         [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "           2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  0,  2,  2,  2]]),\n",
       " tensor([6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 4, 4, 4]),\n",
       " tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 4, 5, 5, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_loader)) # sentence length x batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1057038-3c57-4c24-9be6-ab8c8bc7d518",
   "metadata": {},
   "source": [
    "## 1. Basic RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e27bd58-2d13-4f6f-9962-70f8bdc902fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packing sequences: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, dict_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dict_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    def forward(self, x, lens):\n",
    "        input = self.embedding(x)\n",
    "        input = pack_padded_sequence(input, lens)\n",
    "        output, hidden = self.gru(input, torch.zeros(1, x.shape[1], self.hidden_size))\n",
    "        return output, hidden \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, dict_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(dict_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, dict_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        input = self.embedding(x)\n",
    "        input = F.relu(input)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecc202ac-f97c-422c-9f65-aa4b07012e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "learning_rate = 0.05\n",
    "encoder = EncoderRNN(french_lang.n_words+1, hidden_size)\n",
    "decoder = DecoderRNN(english_lang.n_words+1, hidden_size) #+1 required for padding token (token 0)\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=0) # should not calculate loss if the ith word is a padding token (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b074ecee-9cd6-41bf-a20d-4d67d25b8dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.036394715309143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9837220907211304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9020172357559204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8330768346786499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7824727296829224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7602018713951111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7024868130683899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6708623766899109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.606452226638794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5697213411331177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5355191826820374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5133482813835144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4967331886291504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.45299190282821655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:07<00:00, 43.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.42356744408607483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SOSToken = 1\n",
    "EOSToken = 2\n",
    "\n",
    "def step(x, y, x_lens, y_lens):\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    encoder_outputs, decoder_hidden = encoder(x, x_lens) # last encoder hidden state becomes decoder hidden state\n",
    "    decoder_input = torch.full((1, decoder_hidden.shape[1]), english_lang.word2index['SOS'])\n",
    "    # no teacher forcing for now, could add in later\n",
    "    loss = 0\n",
    "    target_length = max(y_lens)\n",
    "    for i in range(target_length):\n",
    "        # decoder_input: (1 x 32 x 1)\n",
    "        # decoder_hidden: (1 x 32 x 256)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        # decoder_output: (32 x dict_size)\n",
    "        topv, topi = decoder_output.topk(1, dim=1)\n",
    "        decoder_input = topi.transpose(0, 1)\n",
    "        \n",
    "        loss += criterion(decoder_output, y[i, :].squeeze()).sum() \n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "def run_epoch(data_loader):\n",
    "    total_loss = 0\n",
    "    num_iters = 0\n",
    "    for x, y, x_lens, y_lens in tqdm(data_loader):\n",
    "        total_loss += step(x, y, x_lens, y_lens)\n",
    "        num_iters += 1\n",
    "    return total_loss/num_iters\n",
    "\n",
    "def train(data_loader, num_epochs=30):\n",
    "    losses = []\n",
    "    for i in range(num_epochs):\n",
    "        losses.append(run_epoch(data_loader))\n",
    "        print(f\"Loss: {losses[-1]}\")\n",
    "    return losses\n",
    "        \n",
    "losses = train(data_loader, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e3eada5-0618-4985-9332-22d7e9ef441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suis trop affaire . EOS\n",
      "i m too busy . EOS\n",
      "SOS i am too busy too too . . EOS\n",
      "\n",
      "c est une celebre chanteuse . EOS\n",
      "she is a famous singer . EOS\n",
      "SOS she is a famous singer singer . . EOS\n",
      "\n",
      "elles sont chanceuses d etre en vie . EOS\n",
      "they re lucky to be alive . EOS\n",
      "SOS they re lucky to be your alive . EOS\n",
      "\n",
      "je suis desolee de vous avoir derangees ! EOS\n",
      "i m sorry to have bothered you . EOS\n",
      "SOS i m sorry to have bothered you you . EOS\n",
      "\n",
      "je suis fier de mon pere . EOS\n",
      "i m proud of my father . EOS\n",
      "SOS i m proud of my my father . EOS\n",
      "\n",
      "je ne suis pas impressionnee . EOS\n",
      "i m not impressed . EOS\n",
      "SOS i m not impressed impressed . EOS\n",
      "\n",
      "je me sens triste a cause de ca . EOS\n",
      "i am feeling sad about it . EOS\n",
      "SOS i am really sad about about it it . EOS\n",
      "\n",
      "j y songe encore . EOS\n",
      "i m still thinking about it . EOS\n",
      "SOS i m still thinking about it it . EOS\n",
      "\n",
      "elle est deux ans plus vieille que toi . EOS\n",
      "she is two years older than you . EOS\n",
      "SOS she is two years older than you . EOS\n",
      "\n",
      "je suis juste un monsieur tout le monde . EOS\n",
      "i am just a nobody . EOS\n",
      "SOS i am just a to just a teacher . EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_to_lang(sentence, lang):\n",
    "    text = [lang.index2word[sentence[i]] for i in range(len(sentence))]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def eval_on_sentence(x):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    x = x.view(-1, 1)\n",
    "    input = encoder.embedding(x)\n",
    "    x_len = torch.tensor([x.shape[0]])\n",
    "    input = pack_padded_sequence(input, x_len)\n",
    "    encoder_outputs, decoder_hidden = encoder(x, x_len) # last encoder hidden state becomes decoder hidden state\n",
    "    y_pred = [english_lang.word2index['SOS']]\n",
    "    decoder_input = torch.full((1, decoder_hidden.shape[1]), y_pred[-1])\n",
    "\n",
    "    while y_pred[-1]!=EOSToken: # 2 is the EOSToken\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1, dim=1)\n",
    "        decoder_input = topi.transpose(0, 1)\n",
    "        y_pred.append(topi.item())\n",
    "        \n",
    "    return y_pred\n",
    "        \n",
    "def eval_on_sentences(dataset, lang1, lang2, num=10):\n",
    "    for i in range(num):\n",
    "        idx = random.randrange(len(dataset))\n",
    "        x, y = dataset[idx]\n",
    "        y_pred = eval_on_sentence(x)\n",
    "        print(convert_to_lang(x.numpy(), lang1))\n",
    "        print(convert_to_lang(y.numpy(), lang2))\n",
    "        print(convert_to_lang(y_pred, lang2))\n",
    "        print()\n",
    "              \n",
    "eval_on_sentences(dataset, french_lang, english_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972d770-62b6-4cc9-9c17-6a9d18a638dd",
   "metadata": {},
   "source": [
    "## 2. RNNs with Attention\n",
    "https://arxiv.org/pdf/1409.0473.pdf \\\n",
    "https://github.com/lukysummer/Bahdanau-Attention-in-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51774a1d-462c-4c69-8d4f-e1b05f611297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, dict_size, hidden_size):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # 1.\n",
    "        self.embedding = nn.Embedding(dict_size, hidden_size)\n",
    "        # 2.\n",
    "        self.attn_combine = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.attn_weight = nn.Linear(hidden_size, 1)\n",
    "        # 3.\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, dict_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        # input : 1 x 32 x 256\n",
    "        # hidden: 1 x 32 x 256\n",
    "        # encoder_outputs: sentence_len x 32 x 256\n",
    "        # 1. \n",
    "        input = self.embedding(x)\n",
    "        input = F.relu(input)\n",
    "        # 2. \n",
    "        encoder_outputs = pad_packed_sequence(encoder_outputs)[0]\n",
    "        #print(encoder_outputs.shape)\n",
    "        num_outputs = encoder_outputs.shape[0]\n",
    "        combined_vector = torch.cat((hidden.repeat(num_outputs, 1, 1), encoder_outputs), dim=2)\n",
    "        #print(combined_vector.shape)\n",
    "        combined_vector = F.tanh(self.attn_combine(combined_vector)) # sentence_len x 32 x 256\n",
    "        #print(combined_vector.shape)\n",
    "        self.attn_weights = F.softmax(self.attn_weight(combined_vector), dim=0) # sentence_len x 32 x 1\n",
    "        #print(self.attn_weights.shape)\n",
    "        encoder_outputs_t = encoder_outputs.transpose(0, 1).transpose(1, 2) # 32 x 256 x sentence_len\n",
    "        #print(encoder_outputs_t.shape)\n",
    "        context_vector = torch.bmm(encoder_outputs_t, self.attn_weights.transpose(1, 0)) # 32 x 256 x 1\n",
    "        context_vector = context_vector.transpose(1, 2).transpose(0, 1) # 1 x 32 x 256 \n",
    "        #print(context_vector.shape)\n",
    "        # 3. \n",
    "        input = torch.cat((context_vector, input), dim=2)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2aa1c80-4038-4d22-b7b3-3144840dc6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "learning_rate = 0.1\n",
    "#encoder = EncoderRNN(french_lang.n_words+1, hidden_size)\n",
    "#decoder = AttentionDecoderRNN(english_lang.n_words+1, hidden_size) #+1 required for padding token (token 0)\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=0) # should not calculate loss if the ith word is a padding token (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa907e2a-6a56-4777-82ef-f8d0073d108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:15<00:00, 22.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.84750497341156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:14<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.4892609119415283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:14<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3214900493621826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:15<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1736665964126587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:15<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0479902029037476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:15<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9010565876960754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:14<00:00, 22.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7867129445075989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:14<00:00, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6807211637496948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:14<00:00, 22.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6312524080276489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:15<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5505753755569458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:15<00:00, 21.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.46866580843925476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:15<00:00, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4180297255516052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:14<00:00, 22.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.41558676958084106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:15<00:00, 22.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.38648098707199097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 332/332 [00:15<00:00, 22.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3162214756011963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = train(data_loader, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "79937257-6981-44a8-b511-f201c4de0c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tu es plus grande que moi . EOS\n",
      "you are taller than me . EOS\n",
      "SOS you are taller than i than am am . EOS\n",
      "\n",
      "il est toujours alite . EOS\n",
      "he is still on his back . EOS\n",
      "SOS he is always on on his business . EOS\n",
      "\n",
      "je suis en route pour retourner en ville . EOS\n",
      "i m headed back into town . EOS\n",
      "SOS i m on on town of in town . EOS\n",
      "\n",
      "nous allons jouer au tennis . EOS\n",
      "we re going to play tennis . EOS\n",
      "SOS we re going to play tennis tennis . EOS\n",
      "\n",
      "tu es deprimee n est ce pas ? EOS\n",
      "you re depressed aren t you ? EOS\n",
      "SOS you re depressed aren t you ? EOS\n",
      "\n",
      "je ne suis pas bon pour faire semblant . EOS\n",
      "i m not good at pretending . EOS\n",
      "SOS i m not good at do good . . EOS\n",
      "\n",
      "je suis desole de vous avoir deranges ! EOS\n",
      "i m sorry to have bothered you . EOS\n",
      "SOS i m sorry to have bothered you . . EOS\n",
      "\n",
      "je suis vraiment decu . EOS\n",
      "i m really disappointed . EOS\n",
      "SOS i am really disappointed . EOS\n",
      "\n",
      "je ne suis pas ton serviteur . EOS\n",
      "i m not your servant . EOS\n",
      "SOS i m not your of not your . EOS\n",
      "\n",
      "vous n etes pas normales . EOS\n",
      "you re not normal . EOS\n",
      "SOS you re not normal you . EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_on_sentences(dataset, french_lang, english_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ad0d9cd8-8d91-47f4-b541-f6fdf099f87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_attention_matrix(x, y, lang1, lang2):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    x = x.view(-1, 1)\n",
    "    input = encoder.embedding(x)\n",
    "    x_len = torch.tensor([x.shape[0]])\n",
    "    input = pack_padded_sequence(input, x_len)\n",
    "    encoder_outputs, decoder_hidden = encoder(x, x_len) # last encoder hidden state becomes decoder hidden state\n",
    "    y_pred = [english_lang.word2index['SOS']]\n",
    "    decoder_input = torch.full((1, decoder_hidden.shape[1]), y_pred[-1])\n",
    "    attention_weights = []\n",
    "    while y_pred[-1]!=EOSToken: # 2 is the EOSToken\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        attention_weights.append(list(decoder.attn_weights.squeeze().detach().numpy()))\n",
    "        topv, topi = decoder_output.topk(1, dim=1)\n",
    "        decoder_input = topi.transpose(0, 1)\n",
    "        y_pred.append(topi.item())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(attention_weights)\n",
    "    x = [lang1.index2word[x[i, 0].item()] for i in range(len(x))]\n",
    "    y_pred = [lang2.index2word[y_pred[i]] for i in range(len(y_pred))]\n",
    "    ax.set_xticks(range(len(x)), labels=x)\n",
    "    ax.set_yticks(range(len(y_pred)), labels=y_pred)\n",
    "    \n",
    "idx = random.randrange(len(dataset))\n",
    "x, y = dataset[idx]\n",
    "generate_attention_matrix(x, y, french_lang, english_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa6385-c778-49a8-af26-9a99f4ea947f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
